{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bSlQVypwj7Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             precision_recall_curve, f1_score, classification_report,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21138,
     "status": "ok",
     "timestamp": 1759406361950,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "2nI_Z-Plwvef",
    "outputId": "4de0cfd0-3953-4944-8d90-c9538803aaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSAqnJM_wk9R"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset\": \"B\",   # \"A\" or \"B\"\n",
    "\n",
    "    # Dataset A\n",
    "    \"A\": {\n",
    "        \"file\": \"/content/drive/MyDrive/wustl_iiot_2021.csv\",       # your file path\n",
    "        \"cat_cols\": [\"SrcAddr\", \"DstAddr\"],\n",
    "        \"drop_cols\": [\"StartTime\", \"LastTime\", \"Traffic\"],\n",
    "        \"target_col\": \"Target\",\n",
    "        \"d_model\": 128,\n",
    "        \"num_layers\": 3,\n",
    "        \"n_heads\": 8,\n",
    "        \"batch_size\": 256,\n",
    "        \"use_focal\": False,import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             precision_recall_curve, f1_score, classification_report,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "        \"sample_frac\": 0.1,\n",
    "        \"sample_n\": None,        # or take exact number of rows\n",
    "        \"dataset_name\": \"network_attack\"\n",
    "    },\n",
    "\n",
    "    # Dataset B\n",
    "    \"B\": {\n",
    "        \"file\": \"/content/drive/MyDrive/creditcard.csv\",      # credit card fraud dataset path\n",
    "        \"cat_cols\": [],                # no categorical\n",
    "        \"drop_cols\": [],\n",
    "        \"target_col\": \"Class\",\n",
    "        \"d_model\": 64,                 # smaller model (extreme imbalance)\n",
    "        \"num_layers\": 2,\n",
    "        \"n_heads\": 4,\n",
    "        \"batch_size\": 128,\n",
    "        \"use_focal\": True,              # focal loss works well here\n",
    "        \"sample_frac\": 0.5,\n",
    "        \"sample_n\": None,       # or pick exact number of rows\n",
    "        \"dataset_name\": \"fraud_transaction\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2W8w6JmA0PYE"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Helper: stratified sampling\n",
    "# -----------------\n",
    "def stratified_sample(df, target_col, frac=None, n=None, random_state=42):\n",
    "    \"\"\"Return stratified sample keeping target ratio\"\"\"\n",
    "    if frac is not None:\n",
    "        df_sampled = df.groupby(target_col, group_keys=False)\\\n",
    "                       .apply(lambda x: x.sample(frac=frac, random_state=random_state))\n",
    "    elif n is not None:\n",
    "        # allocate samples proportional to class distribution\n",
    "        class_counts = df[target_col].value_counts()\n",
    "        total = class_counts.sum()\n",
    "        df_sampled = []\n",
    "        for c, count in class_counts.items():\n",
    "            take = int(n * (count / total))\n",
    "            df_sampled.append(df[df[target_col] == c].sample(n=take, random_state=random_state))\n",
    "        df_sampled = pd.concat(df_sampled)\n",
    "    else:\n",
    "        df_sampled = df\n",
    "    return df_sampled.sample(frac=1.0, random_state=random_state)  # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5hU3yM6xrS_"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset + preprocessing\n",
    "# -----------------\n",
    "def prepare_dataframes(df, cat_cols, drop_cols, target_col, test_size=0.15, val_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits dataframe into train/val/test with optional column dropping.\n",
    "    - df: input DataFrame\n",
    "    - cat_cols: list of categorical column names\n",
    "    - target_col: anomaly/fraud label column\n",
    "    - drop_cols: list of columns to drop before splitting (default None)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop unwanted columns if provided\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # Handle missing values\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int).values\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    val_relative = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=val_relative, stratify=y_trainval, random_state=random_state\n",
    "    )\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "def fit_transform_preprocessors(X_train, X_val, X_test, cat_cols):\n",
    "    num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val_num = scaler.transform(X_val[num_cols])\n",
    "    X_test_num = scaler.transform(X_test[num_cols])\n",
    "    encoders = {}\n",
    "    if cat_cols:\n",
    "        X_train_cat = np.zeros((len(X_train), len(cat_cols)), dtype=int)\n",
    "        X_val_cat   = np.zeros((len(X_val), len(cat_cols)), dtype=int)\n",
    "        X_test_cat  = np.zeros((len(X_test), len(cat_cols)), dtype=int)\n",
    "        for i, c in enumerate(cat_cols):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(pd.concat([X_train[c], X_val[c], X_test[c]], axis=0).astype(str))\n",
    "            encoders[c] = le\n",
    "            X_train_cat[:, i] = le.transform(X_train[c].astype(str))\n",
    "            X_val_cat[:, i] = le.transform(X_val[c].astype(str))\n",
    "            X_test_cat[:, i] = le.transform(X_test[c].astype(str))\n",
    "    else:\n",
    "        X_train_cat, X_val_cat, X_test_cat = None, None, None\n",
    "    return num_cols, scaler, encoders, (X_train_num, X_val_num, X_test_num), (X_train_cat, X_val_cat, X_test_cat)\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y):\n",
    "        self.X_num = X_num.astype(np.float32)\n",
    "        self.X_cat = X_cat.astype(np.int64) if X_cat is not None else None\n",
    "        self.y = y.astype(np.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        x_num = torch.from_numpy(self.X_num[idx])\n",
    "        x_cat = torch.from_numpy(self.X_cat[idx]) if self.X_cat is not None else torch.zeros(0, dtype=torch.long)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return x_num, x_cat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gmGfxD2ye6F"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Transformer model\n",
    "# -----------------\n",
    "class VanillaTransformer(nn.Module):\n",
    "    def __init__(self, num_num_features, cat_cardinalities, feat_order,\n",
    "                 num_feature_indices, cat_feature_indices,\n",
    "                 d_model=128, n_heads=8, num_layers=3, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        seq_len = len(feat_order)\n",
    "        d_ff = d_ff or d_model * 4\n",
    "        self.feature_embedding = nn.Parameter(torch.randn(seq_len, d_model) * 0.02)\n",
    "        self.value_proj = nn.Linear(1, d_model)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, d_model) * 0.02)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(seq_len + 1, d_model) * 0.02)\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                                   dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        hidden = max(d_model // 2, 32)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        self.num_pos = num_feature_indices\n",
    "        self.cat_pos = cat_feature_indices\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        batch = x_num.shape[0]\n",
    "        seq_len = self.feature_embedding.shape[0]\n",
    "        tokens = self.feature_embedding.unsqueeze(0).expand(batch, -1, -1).clone()\n",
    "        v = x_num.unsqueeze(-1)\n",
    "        vproj = self.value_proj(v)\n",
    "        for i, pos in enumerate(self.num_pos):\n",
    "            tokens[:, pos, :] += vproj[:, i, :]\n",
    "        for j, pos in enumerate(self.cat_pos):\n",
    "            emb = self.cat_embs[j](x_cat[:, j])\n",
    "            tokens[:, pos, :] += emb\n",
    "        cls = self.cls_token.unsqueeze(0).expand(batch, -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "        tokens = tokens + self.pos_embedding.unsqueeze(0)\n",
    "        tokens = self.dropout(tokens)\n",
    "        tokens = tokens.permute(1, 0, 2)\n",
    "        out = self.transformer_encoder(tokens)\n",
    "        cls_out = out[0]\n",
    "        logits = self.classifier(cls_out).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "    def forward_verbose(self, x_num, x_cat):\n",
    "        batch = x_num.shape[0]\n",
    "        seq_len = self.feature_embedding.shape[0]\n",
    "\n",
    "        print(\"\\n=== Raw numeric input ===\")\n",
    "        print(x_num)\n",
    "        if x_cat.numel() > 0:\n",
    "            print(\"\\n=== Raw categorical input ===\")\n",
    "            print(x_cat)\n",
    "\n",
    "        # --- Feature embeddings ---\n",
    "        tokens = self.feature_embedding.unsqueeze(0).expand(batch, -1, -1).clone()\n",
    "        print(\"\\n=== Initial feature embeddings ===\")\n",
    "        print(tokens)\n",
    "\n",
    "        # --- Numeric projection ---\n",
    "        vproj = self.value_proj(x_num.unsqueeze(-1))\n",
    "        print(\"\\n=== Numeric projections ===\")\n",
    "        print(vproj)\n",
    "        for i, pos in enumerate(self.num_pos):\n",
    "            tokens[:, pos, :] += vproj[:, i, :]\n",
    "        print(\"\\n=== After adding numeric projections ===\")\n",
    "        print(tokens)\n",
    "\n",
    "        # --- Categorical embeddings ---\n",
    "        if x_cat.numel() > 0:\n",
    "            for j, pos in enumerate(self.cat_pos):\n",
    "                emb = self.cat_embs[j](x_cat[:, j])\n",
    "                tokens[:, pos, :] += emb\n",
    "        print(\"\\n=== After adding categorical embeddings ===\")\n",
    "        print(tokens)\n",
    "\n",
    "        # --- CLS token + positional embeddings ---\n",
    "        cls = self.cls_token.unsqueeze(0).expand(batch, -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "        tokens = tokens + self.pos_embedding.unsqueeze(0)\n",
    "        tokens = self.dropout(tokens)\n",
    "        print(\"\\n=== Tokens after CLS + positional embeddings ===\")\n",
    "        print(tokens)\n",
    "\n",
    "        # --- Transformer layers (step-by-step) ---\n",
    "        out = tokens.permute(1, 0, 2)  # (seq_len+1, batch, d_model)\n",
    "        for i, layer in enumerate(self.transformer_encoder.layers):\n",
    "            out = layer(out)\n",
    "            print(f\"\\n=== After Transformer layer {i} ===\")\n",
    "            print(out.permute(1, 0, 2))  # (batch, seq_len+1, d_model)\n",
    "\n",
    "        # --- CLS pooling + classifier ---\n",
    "        cls_out = out[0]\n",
    "        logits = self.classifier(cls_out).squeeze(-1)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        print(\"\\n=== CLS pooled representation ===\")\n",
    "        print(cls_out)\n",
    "        print(\"\\n=== Logits ===\")\n",
    "        print(logits)\n",
    "        print(\"\\n=== Probabilities ===\")\n",
    "        print(probs)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27iseICCyv9x"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Losses\n",
    "# -----------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, targets):\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-bce)\n",
    "        focal = self.alpha * (1-pt) ** self.gamma * bce\n",
    "        return focal.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWVwYpNGyzoU"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Training utils\n",
    "# -----------------\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x_num, x_cat, y in dataloader:\n",
    "        x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_num, x_cat)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_num.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    ys, probs = [], []\n",
    "    for x_num, x_cat, y in dataloader:\n",
    "        x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
    "        logits = model(x_num, x_cat)\n",
    "        p = torch.sigmoid(logits).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ys.append(y.numpy())\n",
    "    probs = np.concatenate(probs)\n",
    "    ys = np.concatenate(ys)\n",
    "    rocauc = roc_auc_score(ys, probs)\n",
    "    prauc = average_precision_score(ys, probs)\n",
    "    return {'y': ys, 'probs': probs, 'roc_auc': rocauc, 'pr_auc': prauc}\n",
    "\n",
    "def select_best_threshold(y_val, probs_val):\n",
    "    prec, rec, thresh = precision_recall_curve(y_val, probs_val)\n",
    "    f1s = 2*prec*rec/(prec+rec+1e-8)\n",
    "    best_idx = np.argmax(f1s)\n",
    "    return thresh[best_idx], f1s[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsrPkSnwav15"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Helper: anomaly ratio check\n",
    "# -----------------\n",
    "def check_anomaly_ratio(loader, name=\"\"):\n",
    "    total, anomalies = 0, 0\n",
    "    for _, _, y in loader:   # assuming dataset returns (x_num, x_cat, y)\n",
    "        total += len(y)\n",
    "        anomalies += (y == 1).sum().item()\n",
    "    ratio = anomalies / total if total > 0 else 0\n",
    "    print(f\"{name}: {anomalies}/{total} anomalies ({ratio:.4%})\")\n",
    "    return anomalies, total, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlFPePLdy3D7"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Main training flow\n",
    "# -----------------\n",
    "def main_training_flow(cfg, device='cuda'):\n",
    "    df = pd.read_csv(cfg[\"file\"])\n",
    "\n",
    "    # stratified sampling\n",
    "    if cfg.get(\"sample_frac\") or cfg.get(\"sample_n\"):\n",
    "        df = stratified_sample(df, cfg[\"target_col\"],\n",
    "                               frac=cfg.get(\"sample_frac\"),\n",
    "                               n=cfg.get(\"sample_n\"))\n",
    "        print(f\"Sampled dataset shape: {df.shape}, anomaly ratio={df[cfg['target_col']].mean():.4%}\")\n",
    "\n",
    "    cat_cols = cfg[\"cat_cols\"]\n",
    "    target_col = cfg[\"target_col\"]\n",
    "    drop_cols=cfg.get(\"drop_cols\")\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = prepare_dataframes(df, cat_cols, drop_cols, target_col)\n",
    "    num_cols, scaler, encoders, (X_train_num, X_val_num, X_test_num), (X_train_cat, X_val_cat, X_test_cat) = \\\n",
    "        fit_transform_preprocessors(X_train, X_val, X_test, cat_cols)\n",
    "\n",
    "    train_ds = TabularDataset(X_train_num, X_train_cat if cat_cols else np.zeros((len(X_train),0)), y_train)\n",
    "    val_ds   = TabularDataset(X_val_num,   X_val_cat if cat_cols else np.zeros((len(X_val),0)),   y_val)\n",
    "    test_ds  = TabularDataset(X_test_num,  X_test_cat if cat_cols else np.zeros((len(X_test),0)), y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg[\"batch_size\"], shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    feat_order = list(X_train.columns)\n",
    "    num_feature_indices = [feat_order.index(c) for c in num_cols]\n",
    "    cat_feature_indices = [feat_order.index(c) for c in cat_cols]\n",
    "    cat_cardinalities = [len(encoders[c].classes_) for c in cat_cols]\n",
    "\n",
    "    model = VanillaTransformer(\n",
    "        num_num_features=len(num_cols),\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        feat_order=feat_order,\n",
    "        num_feature_indices=num_feature_indices,\n",
    "        cat_feature_indices=cat_feature_indices,\n",
    "        d_model=cfg[\"d_model\"], n_heads=cfg[\"n_heads\"], num_layers=cfg[\"num_layers\"]\n",
    "    ).to(device)\n",
    "\n",
    "    if cfg[\"use_focal\"]:\n",
    "        criterion = FocalLoss(alpha=0.9, gamma=2)\n",
    "    else:\n",
    "        n_pos, n_neg = (y_train==1).sum(), (y_train==0).sum()\n",
    "        pos_weight = torch.tensor([n_neg/max(1,n_pos)], dtype=torch.float32).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "    best_state, best_val_pr, patience, wait = None, -1.0, 5, 0\n",
    "    for epoch in range(20):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_res = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch:02d} Loss={train_loss:.4f} Val PR-AUC={val_res['pr_auc']:.4f} ROC-AUC={val_res['roc_auc']:.4f}\")\n",
    "        if val_res['pr_auc'] > best_val_pr:\n",
    "            best_val_pr = val_res['pr_auc']\n",
    "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    val_res = evaluate(model, val_loader, device)\n",
    "    best_thresh, best_f1 = select_best_threshold(val_res['y'], val_res['probs'])\n",
    "    print(\"Best threshold:\", best_thresh, \"Val F1:\", best_f1)\n",
    "\n",
    "    test_res = evaluate(model, test_loader, device)\n",
    "    preds = (test_res['probs'] >= best_thresh).astype(int)\n",
    "    print(\"Test PR-AUC:\", test_res['pr_auc'], \"ROC-AUC:\", test_res['roc_auc'])\n",
    "    print(classification_report(test_res['y'], preds, digits=4))\n",
    "    return model, scaler, encoders, best_thresh, (X_train_num, X_val_num, X_test_num,\n",
    "                                               X_train_cat, X_val_cat, X_test_cat,\n",
    "                                               y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NKJowR5bWkM"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Main training flow\n",
    "# -----------------\n",
    "def main_training_flow(cfg, device='cuda'):\n",
    "    # --- 1. Load dataset ---\n",
    "    df = pd.read_csv(cfg[\"file\"])\n",
    "\n",
    "    # optional stratified sampling\n",
    "    if cfg.get(\"sample_frac\") or cfg.get(\"sample_n\"):\n",
    "        df = stratified_sample(df, cfg[\"target_col\"],\n",
    "                               frac=cfg.get(\"sample_frac\"),\n",
    "                               n=cfg.get(\"sample_n\"))\n",
    "        print(f\"Sampled dataset shape: {df.shape}, anomaly ratio={df[cfg['target_col']].mean():.4%}\")\n",
    "\n",
    "    # --- 2. Preprocess / split ---\n",
    "    cat_cols = cfg[\"cat_cols\"]\n",
    "    target_col = cfg[\"target_col\"]\n",
    "    drop_cols  = cfg.get(\"drop_cols\")\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = prepare_dataframes(\n",
    "        df, cat_cols, drop_cols, target_col\n",
    "    )\n",
    "\n",
    "    num_cols, scaler, encoders, \\\n",
    "    (X_train_num, X_val_num, X_test_num), \\\n",
    "    (X_train_cat, X_val_cat, X_test_cat) = fit_transform_preprocessors(\n",
    "        X_train, X_val, X_test, cat_cols\n",
    "    )\n",
    "\n",
    "    # --- 3. Build datasets/loaders ---\n",
    "    train_ds = TabularDataset(X_train_num, X_train_cat if cat_cols else np.zeros((len(X_train),0)), y_train)\n",
    "    val_ds   = TabularDataset(X_val_num,   X_val_cat if cat_cols else np.zeros((len(X_val),0)),   y_val)\n",
    "    test_ds  = TabularDataset(X_test_num,  X_test_cat if cat_cols else np.zeros((len(X_test),0)), y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=cfg[\"batch_size\"], shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=cfg[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # check anomaly ratios\n",
    "    check_anomaly_ratio(train_loader, \"Train\")\n",
    "    check_anomaly_ratio(val_loader, \"Validation\")\n",
    "    check_anomaly_ratio(test_loader, \"Test\")\n",
    "\n",
    "    # --- 4. Model setup ---\n",
    "    feat_order = list(X_train.columns)\n",
    "    num_feature_indices = [feat_order.index(c) for c in num_cols]\n",
    "    cat_feature_indices = [feat_order.index(c) for c in cat_cols]\n",
    "    cat_cardinalities   = [len(encoders[c].classes_) for c in cat_cols]\n",
    "\n",
    "    model = VanillaTransformer(\n",
    "        num_num_features=len(num_cols),\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        feat_order=feat_order,\n",
    "        num_feature_indices=num_feature_indices,\n",
    "        cat_feature_indices=cat_feature_indices,\n",
    "        d_model=cfg[\"d_model\"], n_heads=cfg[\"n_heads\"], num_layers=cfg[\"num_layers\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # --- 5. Loss function ---\n",
    "    if cfg[\"use_focal\"]:\n",
    "        criterion = FocalLoss(alpha=0.9, gamma=2)\n",
    "    else:\n",
    "        n_pos, n_neg = (y_train==1).sum(), (y_train==0).sum()\n",
    "        pos_weight = torch.tensor([n_neg/max(1,n_pos)], dtype=torch.float32).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "    # --- 6. Training loop with early stopping ---\n",
    "    best_state, best_val_pr, patience, wait = None, -1.0, 5, 0\n",
    "    for epoch in range(cfg.get(\"max_epochs\", 30)):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_res = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch:02d} Loss={train_loss:.4f} Val PR-AUC={val_res['pr_auc']:.4f} ROC-AUC={val_res['roc_auc']:.4f}\")\n",
    "        if val_res['pr_auc'] > best_val_pr:\n",
    "            best_val_pr = val_res['pr_auc']\n",
    "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # --- 7. Load best state & threshold selection ---\n",
    "    model.load_state_dict(best_state)\n",
    "    val_res = evaluate(model, val_loader, device)\n",
    "    best_thresh, best_f1 = select_best_threshold(val_res['y'], val_res['probs'])\n",
    "    print(\"Best threshold:\", best_thresh, \"Val F1:\", best_f1)\n",
    "\n",
    "    # --- 8. Final test evaluation ---\n",
    "    test_res = evaluate(model, test_loader, device)\n",
    "    preds = (test_res['probs'] >= best_thresh).astype(int)\n",
    "    print(\"Test PR-AUC:\", test_res['pr_auc'], \"ROC-AUC:\", test_res['roc_auc'])\n",
    "    print(classification_report(test_res['y'], preds, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(test_res['y'], preds)\n",
    "\n",
    "    # Print raw numbers\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Plot nicely\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    disp.plot(cmap=\"Blues\", values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- 9. Save model + preprocessors + loaders metadata ---\n",
    "    drive_folder = '/content/drive/MyDrive/anomaly_models'\n",
    "    os.makedirs(drive_folder, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(drive_folder, f\"transformer_{cfg['dataset_name']}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"✅ Model saved to {model_path}\")\n",
    "\n",
    "    # Save scalers/encoders\n",
    "    joblib.dump(scaler,   os.path.join(drive_folder, f\"scaler_{cfg['dataset_name']}.pkl\"))\n",
    "    joblib.dump(encoders, os.path.join(drive_folder, f\"encoders_{cfg['dataset_name']}.pkl\"))\n",
    "\n",
    "    # Save threshold\n",
    "    with open(os.path.join(drive_folder, f\"threshold_{cfg['dataset_name']}.txt\"), \"w\") as f:\n",
    "        f.write(str(best_thresh))\n",
    "\n",
    "    # Save dataset splits\n",
    "    np.savez_compressed(os.path.join(drive_folder, f\"splits_{cfg['dataset_name']}.npz\"),\n",
    "                        X_train_num=X_train_num, X_val_num=X_val_num, X_test_num=X_test_num,\n",
    "                        X_train_cat=X_train_cat, X_val_cat=X_val_cat, X_test_cat=X_test_cat,\n",
    "                        y_train=y_train, y_val=y_val, y_test=y_test)\n",
    "\n",
    "    # --- 10. Return model, preprocessors, threshold, and splits for inspection ---\n",
    "    return model, scaler, encoders, best_thresh, \\\n",
    "        (X_train_num, X_val_num, X_test_num,\n",
    "          X_train_cat, X_val_cat, X_test_cat,\n",
    "          y_train, y_val, y_test), \\\n",
    "        (train_loader, val_loader, test_loader), \\\n",
    "        (num_cols, feat_order, num_feature_indices, cat_feature_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397142,
     "status": "ok",
     "timestamp": 1759406858394,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "EK9_iJhRy4AJ",
    "outputId": "17c7e115-0a45-4146-97ba-e97374ba86c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-921063323.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(frac=frac, random_state=random_state))\n",
      "/tmp/ipython-input-332726257.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset shape: (142404, 31), anomaly ratio=0.1727%\n",
      "Train: 172/99682 anomalies (0.1725%)\n",
      "Validation: 37/21361 anomalies (0.1732%)\n",
      "Test: 37/21361 anomalies (0.1732%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 Loss=0.0047 Val PR-AUC=0.5400 ROC-AUC=0.9260\n",
      "Epoch 01 Loss=0.0019 Val PR-AUC=0.5509 ROC-AUC=0.9404\n",
      "Epoch 02 Loss=0.0018 Val PR-AUC=0.5803 ROC-AUC=0.9636\n",
      "Epoch 03 Loss=0.0017 Val PR-AUC=0.5935 ROC-AUC=0.9459\n",
      "Epoch 04 Loss=0.0017 Val PR-AUC=0.6025 ROC-AUC=0.9723\n",
      "Epoch 05 Loss=0.0017 Val PR-AUC=0.6290 ROC-AUC=0.9738\n",
      "Epoch 06 Loss=0.0016 Val PR-AUC=0.6455 ROC-AUC=0.9713\n",
      "Epoch 07 Loss=0.0016 Val PR-AUC=0.6510 ROC-AUC=0.9772\n",
      "Epoch 08 Loss=0.0016 Val PR-AUC=0.7706 ROC-AUC=0.9784\n",
      "Epoch 09 Loss=0.0013 Val PR-AUC=0.8017 ROC-AUC=0.9850\n",
      "Epoch 10 Loss=0.0012 Val PR-AUC=0.8118 ROC-AUC=0.9839\n",
      "Epoch 11 Loss=0.0012 Val PR-AUC=0.7863 ROC-AUC=0.9841\n",
      "Epoch 12 Loss=0.0012 Val PR-AUC=0.7965 ROC-AUC=0.9861\n",
      "Epoch 13 Loss=0.0011 Val PR-AUC=0.8143 ROC-AUC=0.9883\n",
      "Epoch 14 Loss=0.0011 Val PR-AUC=0.8264 ROC-AUC=0.9888\n",
      "Epoch 15 Loss=0.0011 Val PR-AUC=0.8131 ROC-AUC=0.9898\n",
      "Epoch 16 Loss=0.0010 Val PR-AUC=0.8316 ROC-AUC=0.9902\n",
      "Epoch 17 Loss=0.0010 Val PR-AUC=0.8407 ROC-AUC=0.9893\n",
      "Epoch 18 Loss=0.0010 Val PR-AUC=0.8477 ROC-AUC=0.9912\n",
      "Epoch 19 Loss=0.0011 Val PR-AUC=0.7728 ROC-AUC=0.9906\n",
      "Epoch 20 Loss=0.0010 Val PR-AUC=0.8186 ROC-AUC=0.9921\n",
      "Epoch 21 Loss=0.0010 Val PR-AUC=0.8689 ROC-AUC=0.9903\n",
      "Epoch 22 Loss=0.0010 Val PR-AUC=0.8106 ROC-AUC=0.9912\n",
      "Epoch 23 Loss=0.0010 Val PR-AUC=0.8059 ROC-AUC=0.9903\n",
      "Epoch 24 Loss=0.0010 Val PR-AUC=0.8311 ROC-AUC=0.9914\n",
      "Epoch 25 Loss=0.0010 Val PR-AUC=0.8111 ROC-AUC=0.9921\n",
      "Epoch 26 Loss=0.0010 Val PR-AUC=0.8616 ROC-AUC=0.9915\n",
      "Early stopping\n",
      "Best threshold: 0.5822136 Val F1: 0.8857142807306123\n",
      "Test PR-AUC: 0.7212228396707309 ROC-AUC: 0.9592649064371068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9995    0.9997    0.9996     21324\n",
      "         1.0     0.8182    0.7297    0.7714        37\n",
      "\n",
      "    accuracy                         0.9993     21361\n",
      "   macro avg     0.9089    0.8647    0.8855     21361\n",
      "weighted avg     0.9992    0.9993    0.9992     21361\n",
      "\n",
      "✅ Model saved to /content/drive/MyDrive/anomaly_models/transformer_fraud_transaction.pt\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# RUN\n",
    "# -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = CONFIG[CONFIG[\"dataset\"]]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, scaler, encoders, thresh, splits, loaders, model_info = main_training_flow(cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHXVB2p4hU5H"
   },
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat, y_train, y_val, y_test = splits\n",
    "train_loader, val_loader, test_loader = loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1759408152304,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "Sm8Ydvskyo2E",
    "outputId": "43bf864e-583c-4236-c421-93d50713ccd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       True_Label  Pred_Prob  Pred_Label\n",
      "0             0.0   0.021429           0\n",
      "1             0.0   0.019984           0\n",
      "2             0.0   0.020034           0\n",
      "3             0.0   0.046974           0\n",
      "4             0.0   0.021561           0\n",
      "...           ...        ...         ...\n",
      "21356         0.0   0.020313           0\n",
      "21357         0.0   0.021065           0\n",
      "21358         0.0   0.020663           0\n",
      "21359         0.0   0.022824           0\n",
      "21360         0.0   0.020495           0\n",
      "\n",
      "[21361 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "all_true, all_probs, all_preds = [], [], []\n",
    "\n",
    "# Loop through entire test set\n",
    "with torch.no_grad():\n",
    "    for x_num_batch, x_cat_batch, y_true_batch in test_loader:\n",
    "        x_num_batch = x_num_batch.to(device)\n",
    "        x_cat_batch = x_cat_batch.to(device)\n",
    "        y_true_batch = y_true_batch.cpu().numpy()   # convert tensor to numpy\n",
    "\n",
    "        logits = model(x_num_batch, x_cat_batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs >= 0.6028694).astype(int)\n",
    "\n",
    "        # ✅ extend with arrays/lists instead of tensor\n",
    "        all_true.extend(y_true_batch.tolist())\n",
    "        all_probs.extend(probs.flatten().tolist())\n",
    "        all_preds.extend(preds.flatten().tolist())\n",
    "\n",
    "# Combine into one DataFrame\n",
    "inspect_df = pd.DataFrame({\n",
    "    \"True_Label\": all_true,\n",
    "    \"Pred_Prob\": all_probs,\n",
    "    \"Pred_Label\": all_preds\n",
    "})\n",
    "\n",
    "print(inspect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1759408291953,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "cKY4aSTK5omT",
    "outputId": "e834ce97-b121-4327-f52c-54ee65df09f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       True_Label  Pred_Prob  Pred_Label\n",
      "1034          1.0   0.668111           1\n",
      "1235          1.0   0.602052           0\n",
      "1539          1.0   0.653075           1\n",
      "2656          1.0   0.664248           1\n",
      "3434          1.0   0.655021           1\n",
      "3896          1.0   0.666337           1\n",
      "4238          1.0   0.666703           1\n",
      "5861          1.0   0.656140           1\n",
      "6275          1.0   0.660363           1\n",
      "6436          1.0   0.021706           0\n",
      "6783          1.0   0.622731           1\n",
      "7006          1.0   0.656908           1\n",
      "7462          1.0   0.631387           1\n",
      "9364          1.0   0.271176           0\n",
      "9472          1.0   0.654865           1\n",
      "10546         1.0   0.640312           1\n",
      "10548         1.0   0.616355           1\n",
      "12429         1.0   0.657639           1\n",
      "12442         1.0   0.606685           1\n",
      "12920         1.0   0.639051           1\n",
      "13188         1.0   0.664591           1\n",
      "13752         1.0   0.653599           1\n",
      "13923         1.0   0.025707           0\n",
      "14141         1.0   0.664090           1\n",
      "14162         1.0   0.244709           0\n",
      "14229         1.0   0.155214           0\n",
      "14341         1.0   0.093615           0\n",
      "15114         1.0   0.663334           1\n",
      "15353         1.0   0.666065           1\n",
      "16552         1.0   0.581492           0\n",
      "17619         1.0   0.665676           1\n",
      "18157         1.0   0.050947           0\n",
      "18952         1.0   0.626066           1\n",
      "19958         1.0   0.024642           0\n",
      "20175         1.0   0.654828           1\n",
      "20701         1.0   0.031337           0\n",
      "21068         1.0   0.654800           1\n"
     ]
    }
   ],
   "source": [
    "# Filter only positive class (True_Label == 1)\n",
    "anomalies_df = inspect_df[inspect_df[\"True_Label\"] == 1]\n",
    "\n",
    "print(anomalies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1759408313489,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "59Zo31ny6Ap7",
    "outputId": "3083ccad-4709-4079-b638-d67537b9e401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       True_Label  Pred_Prob  Pred_Label\n",
      "1235          1.0   0.602052           0\n",
      "6436          1.0   0.021706           0\n",
      "9364          1.0   0.271176           0\n",
      "13923         1.0   0.025707           0\n",
      "14162         1.0   0.244709           0\n",
      "14229         1.0   0.155214           0\n",
      "14341         1.0   0.093615           0\n",
      "16552         1.0   0.581492           0\n",
      "18157         1.0   0.050947           0\n",
      "19958         1.0   0.024642           0\n",
      "20701         1.0   0.031337           0\n"
     ]
    }
   ],
   "source": [
    "# False negatives: True_Label=1 but Pred_Label=0\n",
    "false_negatives = inspect_df[(inspect_df[\"True_Label\"] == 1) & (inspect_df[\"Pred_Label\"] == 0)]\n",
    "\n",
    "print(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1759408408223,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "iETe6SMK6SZM",
    "outputId": "f9049620-4a01-4ee9-d43b-7ddaf9472e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       True_Label  Pred_Prob  Pred_Label\n",
      "5765          0.0   0.656085           1\n",
      "6637          0.0   0.661539           1\n",
      "9544          0.0   0.662388           1\n",
      "13704         0.0   0.641890           1\n",
      "14126         0.0   0.661345           1\n",
      "16031         0.0   0.641890           1\n"
     ]
    }
   ],
   "source": [
    "# False positivies: True_Label=1 but Pred_Label=0\n",
    "false_positivies = inspect_df[(inspect_df[\"True_Label\"] == 0) & (inspect_df[\"Pred_Label\"] == 1)]\n",
    "\n",
    "print(false_positivies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1759408656187,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "2kDb3MpDg09y",
    "outputId": "4c210184-02a4-44f6-df04-0e9605c010b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1034\n",
      "Selected rows for inspection (labels): [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Pick first normal and first anomaly in test set\n",
    "normal_idx = np.where(y_test == 0)[0][0]\n",
    "anomaly_idx = np.where(y_test == 1)[0][0]\n",
    "\n",
    "x_num_sample = torch.tensor(np.vstack([X_test_num[normal_idx], X_test_num[anomaly_idx]]), dtype=torch.float32)\n",
    "\n",
    "# Handle the case where there are no categorical columns\n",
    "#if cfg[\"cat_cols\"]:\n",
    "#    x_cat_sample = torch.tensor(np.vstack([X_test_cat[normal_idx], X_test_cat[anomaly_idx]]), dtype=torch.long)\n",
    "#else:\n",
    "#    x_cat_sample = torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "x_cat_sample = torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "y_sample = y_test[[normal_idx, anomaly_idx]]\n",
    "\n",
    "print(normal_idx)\n",
    "print(anomaly_idx)\n",
    "print(\"Selected rows for inspection (labels):\", y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJKg6Mbi6uq6"
   },
   "outputs": [],
   "source": [
    "num_cols, feat_order, num_feature_indices, cat_feature_indices = model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1759408673890,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "zI_ekkQpiUcT",
    "outputId": "29484f9d-eb35-4113-ece0-5a0deef1a9e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VanillaTransformer(\n",
       "  (value_proj): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (cat_embs): ModuleList()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inspect = VanillaTransformer(\n",
    "    num_num_features=len(num_cols),\n",
    "    #cat_cardinalities=[len(encoders[c].classes_) for c in cat_cols],\n",
    "    cat_cardinalities=[],\n",
    "    feat_order=feat_order,\n",
    "    num_feature_indices=num_feature_indices,\n",
    "    cat_feature_indices=cat_feature_indices,\n",
    "    d_model=cfg[\"d_model\"], n_heads=cfg[\"n_heads\"], num_layers=cfg[\"num_layers\"]\n",
    ")\n",
    "model_inspect.load_state_dict(model.state_dict())  # load trained weights\n",
    "model_inspect.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1759408681662,
     "user": {
      "displayName": "Munkhzaya Bayanbat",
      "userId": "02841352344795048115"
     },
     "user_tz": -480
    },
    "id": "6ovVO5privZn",
    "outputId": "9b4bef6b-9e62-4abe-e784-762d2d79d609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Raw numeric input ===\n",
      "tensor([[-9.4192e-01,  7.6905e-01, -1.3228e-01, -3.4122e-01, -6.1047e-01,\n",
      "         -8.4984e-02, -6.7677e-01,  3.7271e-02, -3.7220e-01, -1.1789e+00,\n",
      "          5.9850e-01, -7.3675e-01, -2.0777e-01,  1.5744e+00, -1.9868e-01,\n",
      "          3.7458e-01,  1.0448e+00,  3.3331e-02, -1.7742e+00,  1.1566e+00,\n",
      "          2.3076e-01,  2.0342e-01,  6.0159e-01, -4.5218e-01, -6.2231e-01,\n",
      "          1.8201e+00, -8.0376e-02, -5.4312e-02, -1.4732e-02, -2.8620e-01],\n",
      "        [-1.1271e+00, -8.4336e+00,  5.1859e+00, -1.2318e+01,  6.7146e+00,\n",
      "         -1.0124e+01, -2.1404e+00, -1.3834e+01,  6.4185e+00, -7.7469e+00,\n",
      "         -1.2997e+01,  5.2000e+00, -1.0924e+01,  1.6841e+00, -9.7586e+00,\n",
      "          3.9569e-01, -1.1347e+01, -2.2858e+01, -1.0022e+01,  3.8056e+00,\n",
      "         -1.9633e+00,  1.6259e+00, -1.5522e+00, -3.7441e+00,  1.1107e+00,\n",
      "         -2.7045e+00, -9.6054e-01, -5.0738e+00, -3.3730e+00,  1.0788e+00]])\n",
      "\n",
      "=== Initial feature embeddings ===\n",
      "tensor([[[-0.0290, -0.0071, -0.0285,  ...,  0.0012,  0.0246, -0.0214],\n",
      "         [-0.0270, -0.0564,  0.0928,  ..., -0.1153,  0.1410,  0.0387],\n",
      "         [-0.0363, -0.0226,  0.0161,  ..., -0.0846,  0.0712, -0.0161],\n",
      "         ...,\n",
      "         [-0.0622, -0.0650,  0.0119,  ..., -0.0791,  0.1208, -0.0225],\n",
      "         [-0.0545, -0.0251,  0.0288,  ..., -0.1093,  0.0717, -0.0228],\n",
      "         [-0.0386,  0.0132,  0.0031,  ..., -0.0339, -0.0123, -0.0035]],\n",
      "\n",
      "        [[-0.0290, -0.0071, -0.0285,  ...,  0.0012,  0.0246, -0.0214],\n",
      "         [-0.0270, -0.0564,  0.0928,  ..., -0.1153,  0.1410,  0.0387],\n",
      "         [-0.0363, -0.0226,  0.0161,  ..., -0.0846,  0.0712, -0.0161],\n",
      "         ...,\n",
      "         [-0.0622, -0.0650,  0.0119,  ..., -0.0791,  0.1208, -0.0225],\n",
      "         [-0.0545, -0.0251,  0.0288,  ..., -0.1093,  0.0717, -0.0228],\n",
      "         [-0.0386,  0.0132,  0.0031,  ..., -0.0339, -0.0123, -0.0035]]])\n",
      "\n",
      "=== Numeric projections ===\n",
      "tensor([[[-0.8075, -0.0317,  0.2677,  ..., -0.6963, -0.5863,  0.3074],\n",
      "         [ 0.4170, -1.4703,  1.2358,  ..., -0.7862, -0.0765,  0.8593],\n",
      "         [-0.2280, -0.7125,  0.7258,  ..., -0.7389, -0.3451,  0.5686],\n",
      "         ...,\n",
      "         [-0.1722, -0.7781,  0.7699,  ..., -0.7430, -0.3218,  0.5937],\n",
      "         [-0.1439, -0.8113,  0.7923,  ..., -0.7450, -0.3100,  0.6065],\n",
      "         [-0.3382, -0.5831,  0.6387,  ..., -0.7308, -0.3909,  0.5189]],\n",
      "\n",
      "        [[-0.9400,  0.1239,  0.1630,  ..., -0.6866, -0.6414,  0.2477],\n",
      "         [-6.1691,  6.2673, -3.9710,  ..., -0.3026, -2.8182, -2.1090],\n",
      "         [ 3.5781, -5.1841,  3.7348,  ..., -1.0183,  1.2393,  2.2840],\n",
      "         ...,\n",
      "         [-3.7646,  3.4424, -2.0701,  ..., -0.4792, -1.8172, -1.0253],\n",
      "         [-2.5473,  2.0123, -1.1077,  ..., -0.5686, -1.3105, -0.4767],\n",
      "         [ 0.6387, -1.7308,  1.4111,  ..., -0.8025,  0.0158,  0.9592]]])\n",
      "\n",
      "=== After adding numeric projections ===\n",
      "tensor([[[-8.3645e-01, -3.8811e-02,  2.3921e-01,  ..., -6.9510e-01,\n",
      "          -5.6161e-01,  2.8600e-01],\n",
      "         [ 3.8999e-01, -1.5267e+00,  1.3286e+00,  ..., -9.0153e-01,\n",
      "           6.4447e-02,  8.9799e-01],\n",
      "         [-2.6436e-01, -7.3508e-01,  7.4195e-01,  ..., -8.2348e-01,\n",
      "          -2.7385e-01,  5.5251e-01],\n",
      "         ...,\n",
      "         [-2.3443e-01, -8.4310e-01,  7.8185e-01,  ..., -8.2201e-01,\n",
      "          -2.0106e-01,  5.7121e-01],\n",
      "         [-1.9842e-01, -8.3642e-01,  8.2111e-01,  ..., -8.5429e-01,\n",
      "          -2.3832e-01,  5.8366e-01],\n",
      "         [-3.7679e-01, -5.6987e-01,  6.4180e-01,  ..., -7.6468e-01,\n",
      "          -4.0325e-01,  5.1543e-01]],\n",
      "\n",
      "        [[-9.6896e-01,  1.1686e-01,  1.3446e-01,  ..., -6.8537e-01,\n",
      "          -6.1677e-01,  2.2628e-01],\n",
      "         [-6.1961e+00,  6.2110e+00, -3.8782e+00,  ..., -4.1794e-01,\n",
      "          -2.6772e+00, -2.0703e+00],\n",
      "         [ 3.5417e+00, -5.2066e+00,  3.7509e+00,  ..., -1.1029e+00,\n",
      "           1.3105e+00,  2.2679e+00],\n",
      "         ...,\n",
      "         [-3.8268e+00,  3.3773e+00, -2.0582e+00,  ..., -5.5824e-01,\n",
      "          -1.6965e+00, -1.0478e+00],\n",
      "         [-2.6018e+00,  1.9872e+00, -1.0790e+00,  ..., -6.7782e-01,\n",
      "          -1.2388e+00, -4.9954e-01],\n",
      "         [ 6.0011e-01, -1.7176e+00,  1.4141e+00,  ..., -8.3641e-01,\n",
      "           3.4091e-03,  9.5571e-01]]])\n",
      "\n",
      "=== After adding categorical embeddings ===\n",
      "tensor([[[-8.3645e-01, -3.8811e-02,  2.3921e-01,  ..., -6.9510e-01,\n",
      "          -5.6161e-01,  2.8600e-01],\n",
      "         [ 3.8999e-01, -1.5267e+00,  1.3286e+00,  ..., -9.0153e-01,\n",
      "           6.4447e-02,  8.9799e-01],\n",
      "         [-2.6436e-01, -7.3508e-01,  7.4195e-01,  ..., -8.2348e-01,\n",
      "          -2.7385e-01,  5.5251e-01],\n",
      "         ...,\n",
      "         [-2.3443e-01, -8.4310e-01,  7.8185e-01,  ..., -8.2201e-01,\n",
      "          -2.0106e-01,  5.7121e-01],\n",
      "         [-1.9842e-01, -8.3642e-01,  8.2111e-01,  ..., -8.5429e-01,\n",
      "          -2.3832e-01,  5.8366e-01],\n",
      "         [-3.7679e-01, -5.6987e-01,  6.4180e-01,  ..., -7.6468e-01,\n",
      "          -4.0325e-01,  5.1543e-01]],\n",
      "\n",
      "        [[-9.6896e-01,  1.1686e-01,  1.3446e-01,  ..., -6.8537e-01,\n",
      "          -6.1677e-01,  2.2628e-01],\n",
      "         [-6.1961e+00,  6.2110e+00, -3.8782e+00,  ..., -4.1794e-01,\n",
      "          -2.6772e+00, -2.0703e+00],\n",
      "         [ 3.5417e+00, -5.2066e+00,  3.7509e+00,  ..., -1.1029e+00,\n",
      "           1.3105e+00,  2.2679e+00],\n",
      "         ...,\n",
      "         [-3.8268e+00,  3.3773e+00, -2.0582e+00,  ..., -5.5824e-01,\n",
      "          -1.6965e+00, -1.0478e+00],\n",
      "         [-2.6018e+00,  1.9872e+00, -1.0790e+00,  ..., -6.7782e-01,\n",
      "          -1.2388e+00, -4.9954e-01],\n",
      "         [ 6.0011e-01, -1.7176e+00,  1.4141e+00,  ..., -8.3641e-01,\n",
      "           3.4091e-03,  9.5571e-01]]])\n",
      "\n",
      "=== Tokens after CLS + positional embeddings ===\n",
      "tensor([[[ 3.1552e-02, -9.4942e-03,  4.4920e-02,  ..., -2.9103e-02,\n",
      "           2.6179e-02, -2.1847e-02],\n",
      "         [-8.3132e-01, -2.6730e-02,  1.9462e-01,  ..., -7.4447e-01,\n",
      "          -5.6342e-01,  2.3886e-01],\n",
      "         [ 3.2545e-01, -1.6025e+00,  1.3961e+00,  ..., -9.8804e-01,\n",
      "           1.7540e-01,  8.8986e-01],\n",
      "         ...,\n",
      "         [-3.1404e-01, -9.5980e-01,  8.0144e-01,  ..., -9.2801e-01,\n",
      "          -6.6963e-02,  5.1028e-01],\n",
      "         [-2.4669e-01, -8.7775e-01,  8.3202e-01,  ..., -9.8894e-01,\n",
      "          -1.3822e-01,  5.9236e-01],\n",
      "         [-4.2196e-01, -5.4116e-01,  6.2412e-01,  ..., -8.0818e-01,\n",
      "          -4.0912e-01,  4.8644e-01]],\n",
      "\n",
      "        [[ 3.1552e-02, -9.4942e-03,  4.4920e-02,  ..., -2.9103e-02,\n",
      "           2.6179e-02, -2.1847e-02],\n",
      "         [-9.6382e-01,  1.2894e-01,  8.9863e-02,  ..., -7.3474e-01,\n",
      "          -6.1858e-01,  1.7914e-01],\n",
      "         [-6.2607e+00,  6.1352e+00, -3.8107e+00,  ..., -5.0445e-01,\n",
      "          -2.5663e+00, -2.0785e+00],\n",
      "         ...,\n",
      "         [-3.9064e+00,  3.2606e+00, -2.0386e+00,  ..., -6.6424e-01,\n",
      "          -1.5624e+00, -1.1088e+00],\n",
      "         [-2.6501e+00,  1.9459e+00, -1.0680e+00,  ..., -8.1247e-01,\n",
      "          -1.1387e+00, -4.9084e-01],\n",
      "         [ 5.5494e-01, -1.6889e+00,  1.3964e+00,  ..., -8.7991e-01,\n",
      "          -2.4526e-03,  9.2673e-01]]])\n",
      "\n",
      "=== After Transformer layer 0 ===\n",
      "tensor([[[-0.1223,  0.6500,  0.1016,  ..., -1.6480,  2.1138, -0.6156],\n",
      "         [-0.7420,  0.4213,  0.3188,  ..., -1.7886,  0.6024, -0.0320],\n",
      "         [ 0.1603, -0.9642,  1.4733,  ..., -1.6626,  1.5696,  0.9215],\n",
      "         ...,\n",
      "         [-0.4969, -0.5042,  0.8831,  ..., -1.9548,  1.4581,  0.4539],\n",
      "         [-0.3547, -0.3733,  0.9146,  ..., -1.9609,  1.2934,  0.5425],\n",
      "         [-0.5625, -0.0136,  0.7140,  ..., -1.8999,  0.8804,  0.3643]],\n",
      "\n",
      "        [[ 0.8165,  1.1540,  1.2298,  ..., -0.5815, -1.4429,  0.0738],\n",
      "         [-0.2008,  0.4276,  1.1025,  ..., -0.3566, -0.1710,  0.0883],\n",
      "         [ 0.0471,  1.3051,  0.8236,  ..., -0.5271, -0.7056, -0.3534],\n",
      "         ...,\n",
      "         [ 0.1514,  1.1063,  1.1041,  ..., -0.4905, -0.6244, -0.1762],\n",
      "         [ 0.1615,  0.9208,  1.2093,  ..., -0.4390, -0.5580, -0.0254],\n",
      "         [-1.0469,  0.3527,  0.8413,  ..., -1.1210,  0.3465, -0.6979]]])\n",
      "\n",
      "=== After Transformer layer 1 ===\n",
      "tensor([[[-0.5432, -0.5406, -1.1912,  ..., -0.3414,  1.8797, -0.6207],\n",
      "         [-0.6954, -0.8463, -1.2009,  ..., -0.6345,  1.5557, -0.3557],\n",
      "         [-0.3115, -1.5338, -0.5677,  ..., -0.4294,  1.9776,  0.0804],\n",
      "         ...,\n",
      "         [-0.5760, -1.3036, -0.9005,  ..., -0.6445,  1.9238, -0.1593],\n",
      "         [-0.4965, -1.2838, -0.8883,  ..., -0.6590,  1.8834, -0.0954],\n",
      "         [-0.5937, -1.0663, -0.9986,  ..., -0.6738,  1.6968, -0.1568]],\n",
      "\n",
      "        [[ 1.3567,  1.4188,  1.6425,  ..., -0.9109, -1.5616,  0.8501],\n",
      "         [ 1.0498,  1.1237,  1.6818,  ..., -0.7810, -1.1263,  0.9957],\n",
      "         [ 1.0918,  1.5925,  1.6379,  ..., -0.8394, -1.3231,  0.7384],\n",
      "         ...,\n",
      "         [ 1.1432,  1.4733,  1.7119,  ..., -0.8281, -1.2891,  0.8229],\n",
      "         [ 1.1592,  1.3619,  1.7294,  ..., -0.8059, -1.2640,  0.8909],\n",
      "         [ 0.5822,  1.2085,  1.6812,  ..., -1.2265, -0.9697,  0.7613]]])\n",
      "\n",
      "=== CLS pooled representation ===\n",
      "tensor([[-5.4323e-01, -5.4058e-01, -1.1912e+00,  7.5544e-01, -5.0383e-01,\n",
      "          1.8220e-03,  1.3299e+00,  1.4167e-01,  8.3216e-01,  1.1360e+00,\n",
      "          1.0978e+00, -9.7660e-01,  7.8449e-01,  1.5877e+00, -8.5982e-01,\n",
      "          4.5537e-01, -1.6184e+00,  4.2286e-01, -1.8013e+00, -8.0605e-01,\n",
      "          3.2615e-01, -3.8001e-01, -8.5419e-01,  1.3003e+00, -1.5341e+00,\n",
      "         -1.6980e+00, -5.3951e-01,  4.4665e-01, -1.3616e+00, -3.3043e-01,\n",
      "         -1.9838e-01,  2.5649e-02, -4.0004e-01,  9.8298e-01,  9.8817e-01,\n",
      "          8.5961e-01,  5.3033e-01, -1.1858e+00,  7.9962e-01,  1.1613e+00,\n",
      "         -3.3260e-01, -7.3386e-01,  1.6220e+00,  8.3236e-01,  3.8583e-01,\n",
      "         -8.6533e-01, -3.5762e-01,  8.3870e-01,  7.9394e-01,  3.1816e-02,\n",
      "         -2.3881e+00,  1.0776e-01, -1.4184e+00,  6.4398e-01, -9.5894e-01,\n",
      "          6.4641e-01, -2.1009e-02,  3.3708e-01,  1.0660e+00, -1.5838e+00,\n",
      "          1.6879e+00, -3.4140e-01,  1.8797e+00, -6.2071e-01],\n",
      "        [ 1.3567e+00,  1.4188e+00,  1.6425e+00, -5.8936e-01, -1.9528e-02,\n",
      "         -8.9018e-01, -3.2360e-01,  5.4814e-01, -7.7268e-01, -1.0606e+00,\n",
      "         -1.6256e+00,  1.2499e+00, -2.0758e-01, -1.9659e+00,  6.6692e-01,\n",
      "          3.9147e-01,  1.2434e+00, -4.7239e-01,  7.9794e-01,  6.0752e-02,\n",
      "          1.3553e-01, -7.5020e-01,  2.6643e-02, -1.4529e+00, -1.4580e-01,\n",
      "          2.0404e+00,  1.0387e+00,  6.8105e-01, -7.3998e-01,  6.9933e-01,\n",
      "         -1.4735e+00,  1.2265e-01,  3.0875e+00, -1.4131e-01, -1.0861e+00,\n",
      "          2.2593e-01, -2.6583e-01,  1.1474e+00, -6.4619e-01, -1.6401e+00,\n",
      "          6.8503e-01,  5.7443e-01, -1.0981e+00, -3.3592e-01,  6.4181e-01,\n",
      "         -5.4469e-01,  2.5598e-01, -1.2346e+00, -7.8573e-01, -1.2975e+00,\n",
      "         -1.9815e-01, -1.9369e-01,  1.1824e+00,  4.6177e-01,  6.7417e-01,\n",
      "          9.7458e-01,  3.5996e-01, -3.5017e-01, -6.7818e-01,  3.9111e-01,\n",
      "         -3.8209e-02, -9.1090e-01, -1.5616e+00,  8.5006e-01]])\n",
      "\n",
      "=== Logits ===\n",
      "tensor([-3.8213,  0.6997])\n",
      "\n",
      "=== Probabilities ===\n",
      "tensor([0.0214, 0.6681])\n",
      "\n",
      "Predicted probabilities (normal vs anomaly): [0.02142897 0.6681111 ]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    probs = model_inspect.forward_verbose(x_num_sample, x_cat_sample)\n",
    "\n",
    "print(\"\\nPredicted probabilities (normal vs anomaly):\", probs.numpy())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2S4N55i+T+w6/HqB+NBu0",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
